{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Overview\n",
    "\n",
    "# Objective\n",
    "\n",
    "The objective of this project is for you to demonstrate your mastery of the Machine Learning process\n",
    "**using Neural Networks**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission requirements\n",
    "\n",
    "The guidelines will be similar to the Midterm\n",
    "- you will write a procedure that takes raw data and produces predictions\n",
    "\n",
    "You will submit a *single* model for evaluation.\n",
    "\n",
    "**Demonstrate that all cells in your notebook work**\n",
    "\n",
    "The final cell in your notebook should print the message \"Done\"\n",
    "- `print(\"Done\")`\n",
    "- If we run your notebook and this last cell does not execute your submission will be inadequate\n",
    "\n",
    "## Testing\n",
    "\n",
    "*You must perform out of sample testing*.\n",
    "\n",
    "If you want to perform cross-validation in training, that is fine, but you\n",
    "must *also* test out of sample to show that you are not over-fitting.\n",
    "\n",
    "It is up to you to create the out of sample data that you feel best evaluates your model.\n",
    "\n",
    "We will create holdout data (that we will not show you) for grading.\n",
    "\n",
    "The procedure you write to make predictions should be able to work on the unseen holdout data\n",
    "(similar to how it should work for your test set but the holdout set has *no targets*)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data\n",
    "\n",
    "Data will be provided to you \n",
    "- as multiple files in a directory which we refer to as a *data directory*\n",
    "\n",
    "The reason for this is that the different files may convey different information.\n",
    "\n",
    "You will be responsible for deciding\n",
    "- which files to use\n",
    "- which fields within the files to use\n",
    "\n",
    "We will give you a data directory for training.\n",
    "\n",
    "# Submission guidelines\n",
    "\n",
    "Here are the basics, a code template that you must complete is in the following cells\n",
    "- you will be required to store  your model in a file\n",
    "- you will be required to write a procedure `MyModel` that takes two arguments\n",
    "    - `test_dir`\n",
    "        - this is a *relative path* to the holdout data directory\n",
    "    - `model_path`\n",
    "        - this is a *relative path* to the file containing your model\n",
    "- the holdout data directory is similar in structure to the training data directory\n",
    "    - but without target labels !  It is your job to predict these.\n",
    "- your procedure must produce predictions given this holdout data directory\n",
    "\n",
    "This means that your procedure must\n",
    "- prepare the files in the holdout data directory similar to the way that they were prepared in the training data directory\n",
    "\n",
    "We will provide you with a sample data directory that will resemble the holdout -- this is so that you\n",
    "may test the procedure you write for submission.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed submission guidelines\n",
    "\n",
    "\n",
    "In **addition to your notebook that trains/evaluates your model**, \n",
    "- please also submit an **archive file of the directory** whose name is stored in `model_path`, which \n",
    "contains your trained model.\n",
    "    - use `saveModel` to put your final, trained model in this directory\n",
    "- We will **not** train your model; we will only use the method `MyModel`\n",
    "    - which **you** will implement\n",
    "    - and which uses `loadModel` and the directory whose name is stored in `model_path`\n",
    "    - this will create the model that we will evaluate\n",
    "\n",
    "\n",
    "Here is a code template for you to complete\n",
    "- it will save your model (assuming it is in variable `my_model`)\n",
    "- it provides the specification for procedure `MyModel`, which *you must complete*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\") \n",
    "\n",
    "import sklearn\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "cross_validation_k = 5\n",
    "\n",
    "modelName = \"final_model\"\n",
    "model_path = os.path.join(\".\", modelName)\n",
    "\n",
    "def saveModel(model, model_path): \n",
    "    try:\n",
    "        os.makedirs(model_path)\n",
    "    except OSError:\n",
    "        print(\"Directory {dir:s} already exists, files will be over-written.\".format(dir=model_path))\n",
    "        \n",
    "    # Save JSON config to disk\n",
    "    json_config = model.to_json()\n",
    "    with open(os.path.join(model_path, 'config.json'), 'w') as json_file:\n",
    "        json_file.write(json_config)\n",
    "    # Save weights to disk\n",
    "    model.save_weights(os.path.join(model_path, 'weights.h5'))\n",
    "    \n",
    "    print(\"Model saved in directory {dir:s}; create an archive of this directory and submit with your assignment.\".format(dir=model_path))\n",
    "    \n",
    "def loadModel(model_path):\n",
    "    # Reload the model from the 2 files we saved\n",
    "    with open(os.path.join(model_path, 'config.json')) as json_file:\n",
    "        json_config = json_file.read()\n",
    "    model = tf.keras.models.model_from_json(json_config)\n",
    "    model.load_weights(os.path.join(model_path, 'weights.h5'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def MyModel(test_dir, model_path):\n",
    "    # YOU MAY NOT change model after this statement !\n",
    "    model = loadModel(model_path)\n",
    "    \n",
    "    # It should run model to create an array of predictions; we initialize it to the empty array for convenience\n",
    "    predictions = []\n",
    "    \n",
    "    # We need to match your array of predictions with the examples you are predicting\n",
    "    # The array below (ids) should have a one-to-one correspondence and identify the example your are predicting\n",
    "    # For Bankruptcy: the Id column\n",
    "    # For Stock prediction: the date on which you are making a prediction\n",
    "    ids = []\n",
    "    \n",
    "    # YOUR CODE GOES HERE\n",
    "    trade_df, all_df = load_data(holdout_dir)\n",
    "    X_train_all, X_test, y_train_all, y_test = train_test_split(all_df)\n",
    "    X_train_all, X_test = imputation(X_train_all, X_test)\n",
    "    useful_tickers = [\"AAPL\",\"XLK\",\"SPY\",\"XLY\",\"XLB\",\"XLI\",\"GOOG\",\"IBM\",\"MA\"]\n",
    "    X_train_all,X_test = feature_engineering(X_train_all, X_test, useful_tickers)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_all_scale = pd.DataFrame(scaler.fit_transform(X_train_all), columns=X_train_all.columns)\n",
    "    X_test_scale = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "    X_train_all_scale = pd.DataFrame(scaler.fit_transform(X_train_all), columns=X_train_all.columns)\n",
    "    X_test_scale = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "    \n",
    "    return_feature_columns = ['Return_'+ticker for ticker in useful_tickers]\n",
    "    vol_feature_columns = ([f'Vol_{window}_{ticker}' for ticker in useful_tickers for window in [20, 60, 250]] \n",
    "                                + [f'Range_{window}_{ticker}' for ticker in useful_tickers for window in [20, 60, 250]])\n",
    "    volume_feature_columns = ['log(Volume)_'+ticker for ticker in useful_tickers]\n",
    "    used_features = return_feature_columns + vol_feature_columns + volume_feature_columns\n",
    "\n",
    "    model_test, y_test_predict = test_final_model(X_train_all_scale, X_test_scale, y_train_all, y_test, model, 'final model', used_features)\n",
    "    prediction = y_test_predict\n",
    "    ids = list(y_test.index)\n",
    "    \n",
    "    print(y_test_predict)\n",
    "    return predictions, ids\n",
    "\n",
    "def load_data(data_dir):\n",
    "    \n",
    "    all_tickers = []\n",
    "    filenames = os.listdir(data_dir)\n",
    "    for filename in filenames:\n",
    "        all_tickers.append(filename.split('.')[0])\n",
    "    \n",
    "    trade_ticker = 'AAPL'\n",
    "    \n",
    "    trade_df = pd.read_csv(os.path.join(data_dir, f'{trade_ticker}.csv')).set_index('Dt')\n",
    "    trade_df.index = pd.to_datetime(trade_df.index)\n",
    "    trade_df['Return'] = trade_df['Close'].pct_change(periods=1)\n",
    "    trade_df['Adj Return'] = trade_df['Adj Close'].pct_change(periods=1)\n",
    "    \n",
    "\n",
    "    all_df = trade_df.copy()\n",
    "    \n",
    "    \n",
    "    # merge DataFrame\n",
    "    for ticker in all_tickers:\n",
    "        if ticker == trade_ticker:\n",
    "            continue\n",
    "        new_df = pd.read_csv(data_dir + ticker + '.csv').set_index(\"Dt\")\n",
    "        new_df.index = pd.to_datetime(new_df.index)\n",
    "        new_df['Return'] = new_df['Close'].pct_change(periods=1)\n",
    "        #all_df = all_df.join(new_df, how='left', rsuffix=f'_{ticker}') \n",
    "        new_df[\"Adj Return\"] =new_df[\"Adj Close\"].pct_change(1)\n",
    "        all_df = all_df.join(new_df, how = \"left\", rsuffix = \"_\" + str(ticker))\n",
    "           \n",
    "    all_df.columns = [col+'_AAPL' for col in all_df.columns[:11]] + list(all_df.columns[11:])\n",
    "    all_df['Forward_AAPL'] = all_df['Adj Close_AAPL'].pct_change(periods=1).shift(-1)\n",
    "    return trade_df, all_df\n",
    "def train_test_split(all_df):\n",
    "\n",
    "    X, y = all_df.iloc[:,:-1].astype('float64'), all_df['Forward_AAPL'].astype('float64')\n",
    "\n",
    "    X_train_all, X_test = X.iloc[: -200, :], X.iloc[-200:-1, :]\n",
    "    y_train_all, y_test = y.iloc[: -200], y.iloc[-200:-1]\n",
    "    return X_train_all, X_test, y_train_all, y_test\n",
    "\n",
    "def imputation(X_train_all, X_test):\n",
    "    # imputation mean values\n",
    "    mean_univariate_columns = [col for col in X_train_all.columns if col.split('_')[0] in ['Volume']]\n",
    "    zero_univariate_columns = [col for col in X_train_all.columns if col.split('_')[0] in ['Return', 'Div']]\n",
    "    one_univariate_columns = [col for col in X_train_all.columns if col.split('_')[0] in ['Factor']]\n",
    "    first_univariate_columns = [col for col in X_train_all.columns if col.split('_')[0].split(' ')[0] in ['Adj Close', 'Open', 'High', 'Low', 'Close']]\n",
    "\n",
    "\n",
    "    # imputation with mean\n",
    "    imputer_uni_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    X_train_all[mean_univariate_columns] = imputer_uni_mean.fit_transform(X_train_all[mean_univariate_columns])\n",
    "    X_test[mean_univariate_columns] = imputer_uni_mean.transform(X_test[mean_univariate_columns])\n",
    "\n",
    "    # imputation with zero\n",
    "    imputer_uni_zero = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "    X_train_all[zero_univariate_columns] = imputer_uni_zero.fit_transform(X_train_all[zero_univariate_columns])\n",
    "    X_test[zero_univariate_columns] = imputer_uni_zero.transform(X_test[zero_univariate_columns])\n",
    "\n",
    "    # imputation with one\n",
    "    imputer_uni_one = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=1)\n",
    "    X_train_all[one_univariate_columns] = imputer_uni_one.fit_transform(X_train_all[one_univariate_columns])\n",
    "    X_test[one_univariate_columns] = imputer_uni_one.transform(X_test[one_univariate_columns])\n",
    "\n",
    "    # imputation with first value\n",
    "    X_train_all[first_univariate_columns] = X_train_all[first_univariate_columns].fillna(method='bfill')\n",
    "\n",
    "    return X_train_all, X_test\n",
    "\n",
    "def feature_engineering(X_train_all, X_test, useful_tickers):\n",
    "    X_all = pd.concat([X_train_all, X_test])\n",
    "    \n",
    "    # technical\n",
    "    for ticker in useful_tickers:\n",
    "        for window in [7, 14, 28]:\n",
    "            X_all[f'RSI_{window}_{ticker}'] = RSI(X_all['Adj Close_'+ticker], window)\n",
    "            X_all[f'OBV_{window}_{ticker}'] = OBV(X_all['Adj Close_'+ticker], X_all['Volume_'+ticker], window)\n",
    "    \n",
    "    # volume\n",
    "    for ticker in useful_tickers:\n",
    "        X_all['log(Volume)_'+ticker] = np.log(X_all['Volume_'+ticker])\n",
    "    \n",
    "    \n",
    "    # div/split\n",
    "    div_event = X_all['Div_AAPL']>0\n",
    "    split_event = X_all['Factor_AAPL']>1\n",
    "\n",
    "    X_all['is_div'] = X_all['Div_AAPL']>0\n",
    "    X_all['is_split'] = X_all['Factor_AAPL']>1\n",
    "    \n",
    "    # volatility\n",
    "    for ticker in useful_tickers:\n",
    "        for window in [20, 60, 250]:\n",
    "            X_all[f'Vol_{window}_{ticker}'] = X_all['Return_'+ticker].rolling(window).std().fillna(0)\n",
    "            X_all[f'Range_{window}_{ticker}'] = ((X_all['High_'+ticker] - X_all['Low_'+ticker])/X_all['Close_'+ticker]).rolling(window).mean().fillna(0)\n",
    "    \n",
    "    X_train_all, X_test = X_all.iloc[:-200, :], X_all.iloc[-200:, :]\n",
    "    \n",
    "    return X_train_all, X_test\n",
    "\n",
    "def RSI(price, window):\n",
    "    change = price.diff(1).fillna(0)\n",
    "    is_gain, is_loss = change > 0, change < 0\n",
    "    gain, loss = change, -change\n",
    "    gain[is_loss] = 0\n",
    "    loss[is_gain] = 0\n",
    "    rs = gain.ewm(span=window).mean().fillna(0) / loss.ewm(span=window).mean().fillna(0)\n",
    "    rsi = (0.5 - (1 / (1 + rs))).fillna(0)\n",
    "    return rsi\n",
    "\n",
    "def OBV(price, volume, window):\n",
    "    change = price.diff(1).fillna(0)\n",
    "    return (np.sign(change) * volume).ewm(span=window).mean().fillna(0)\n",
    "\n",
    "def test_final_model(X_train_all_scale, X_test_scale, y_train_all, y_test, model_test, model_name_test, used_features, loss='mean_squared_error'): \n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)    \n",
    "    X_train_all_rnn, y_train_all_rnn, X_test_rnn, y_test_rnn, X_train_rnn, y_train_rnn, X_valid_rnn, y_valid_rnn = transform_dataset(X_train_all_scale[used_features], X_test_scale[used_features], y_train_all, y_test, 100, 1, verbose=False)\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(lr=0.005)\n",
    "    model_test.compile(loss=loss, optimizer=optimizer, metrics='mean_squared_error')\n",
    "\n",
    "\n",
    "    score_test = model_test.evaluate(X_test_rnn, y_test_rnn, verbose=0)\n",
    "    y_test_predict = model_test.predict(X_test_rnn)\n",
    "    r_square = r2_score(y_test_rnn, y_test_predict)\n",
    "    corr = np.corrcoef(y_test_rnn.T, y_test_predict.T)[0,1]\n",
    "    \n",
    "    num_parameters = model_test.count_params()\n",
    "\n",
    "    print('{n:s}: Test Loss: {l:3.4%}, MSE: {m:3.4%}, R^2: {r:3.4%}, Corr: {c:3.4%}'.format(n=model_name_test, l=score_test[0], m=score_test[1], r=r_square, c=corr))\n",
    "    print('Parameters number in model:', num_parameters)\n",
    "    return model_test, y_test_predict\n",
    "\n",
    "def transform_dataset(train_set, test_set, y_train, y_test, n_input, n_output, verbose=True):\n",
    "    all_data = np.vstack((train_set, test_set)).astype('float64')\n",
    "    y_set = y_train.append(y_test).astype('float64')\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(all_data.shape[0] - n_input - n_output + 2):\n",
    "        X_sample = all_data[i:i + n_input, :]\n",
    "        y_sample = y_set[i + n_input - 1:i + n_input - 1 + n_output]\n",
    "        X.append(X_sample)\n",
    "        y.append(y_sample)   \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)    \n",
    "    train_all_X = X[:train_set.shape[0] - n_input + 1, :, :]\n",
    "    train_all_y = y[:train_set.shape[0] - n_input + 1, :]\n",
    "    test_X = X[train_set.shape[0] - n_input + 1:, :, :]\n",
    "    test_y = y[train_set.shape[0] - n_input + 1:, :]\n",
    "    \n",
    "    num_train = int(train_all_X.shape[0] * 0.9)\n",
    "    \n",
    "    train_X, valid_X = train_all_X[:num_train], train_all_X[num_train:]\n",
    "    train_y, valid_y = train_all_y[:num_train], train_all_y[num_train:]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'X_train_all_rnn shape: {train_all_X.shape}')\n",
    "        print(f'X_train_rnn shape: {train_X.shape}')\n",
    "        print(f'X_valid_rnn shape: {valid_X.shape}')\n",
    "        print(f'X_test_rnn shape: {test_X.shape}')\n",
    "\n",
    "        print(f'y_train_all_rnn shape: {train_all_y.shape}')\n",
    "        print(f'y_train_rnn shape: {train_y.shape}')\n",
    "        print(f'y_valid_rnn shape: {valid_y.shape}')\n",
    "        print(f'y_test_rnn shape: {test_y.shape}')\n",
    "    \n",
    "    return train_all_X, train_all_y, test_X, test_y, train_X, train_y, valid_X, valid_y\n",
    "\n",
    "# Assign to variable my_model the model that is your final model (the one  you will be evaluated on)\n",
    "my_model = \"Final Model\"\n",
    "\n",
    "#saveModel(my_model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate your model on the holdout data directory\n",
    "\n",
    "**You must run the following cell** from the directory that contains your model file\n",
    "\n",
    "Here is how we will evaluate your submission\n",
    "- we will create a directory whose only content is\n",
    "    - sub-directory `Data`\n",
    "- we will copy your model file to this directory with the name stored in `model_path`\n",
    "- we will run the cell in your notebook that should be a copy of the one below\n",
    "    - it calls procedure `MyModel` with the arguments given below\n",
    "    - your implementation of `MyModel`\n",
    "        - must successfully load your model file, *given where **we** have place it as described above*\n",
    "        - must successfully return one prediction for each example in the holdout directory *given where **we** have placed the holdout directory*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final model: Test Loss: 0.0135%, MSE: 0.0135%, R^2: -1.9709%, Corr: -9.5251%\n",
      "Parameters number in model: 3051\n",
      "[[9.5037068e-04]\n",
      " [9.5661380e-04]\n",
      " [9.1180031e-04]\n",
      " [6.9428346e-04]\n",
      " [4.1659654e-04]\n",
      " [1.6480929e-04]\n",
      " [1.7510564e-04]\n",
      " [2.4387107e-04]\n",
      " [4.4860493e-04]\n",
      " [8.1398472e-04]\n",
      " [1.1344305e-03]\n",
      " [1.3050594e-03]\n",
      " [1.2980780e-03]\n",
      " [1.2412587e-03]\n",
      " [1.1838038e-03]\n",
      " [1.4835986e-03]\n",
      " [1.6692167e-03]\n",
      " [1.7501276e-03]\n",
      " [1.9981484e-03]\n",
      " [2.4539151e-03]\n",
      " [2.3831320e-03]\n",
      " [2.4313454e-03]\n",
      " [2.1785665e-03]\n",
      " [1.9734283e-03]\n",
      " [1.9685074e-03]\n",
      " [1.7359196e-03]\n",
      " [1.6249027e-03]\n",
      " [1.5334247e-03]\n",
      " [1.6193867e-03]\n",
      " [1.6661806e-03]\n",
      " [1.5943178e-03]\n",
      " [1.3590676e-03]\n",
      " [1.1512742e-03]\n",
      " [1.0577904e-03]\n",
      " [9.7055751e-04]\n",
      " [7.8959658e-04]\n",
      " [5.9961784e-04]\n",
      " [4.0577247e-04]\n",
      " [3.5612797e-04]\n",
      " [4.7699921e-04]\n",
      " [5.6086486e-04]\n",
      " [4.5325712e-04]\n",
      " [3.6191160e-04]\n",
      " [2.2595143e-04]\n",
      " [1.4900789e-04]\n",
      " [4.3107779e-05]\n",
      " [6.4730295e-05]\n",
      " [7.5951335e-05]\n",
      " [8.0452301e-05]]\n"
     ]
    }
   ],
   "source": [
    "holdout_dir = os.path.join(\".\", \"Data\", \"holdout/\")\n",
    "predicts = MyModel(holdout_dir, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
